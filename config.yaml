dir:
  # results: 'results_final/'
  results: 'results_all6/'
  dataset: '../data_own/'

shape_dataset:
  id: sub-221CR_ses-20190515T160400 # my
  # id: sub-221CR_ses-20190514T142312
  # id: sub-231CR_ses-20190924T161413 # abhishek
  # id: sub-231CR_ses-20190921T144923
  # id: sub-KM131_ses-20180117T194023
  # id: sub-267CR_ses-20200331T161053
  # id: sub-245CR_ses-20200331T142427
  # id: sub-KF134_ses-20180208T171905

  win_len: 0.1 # in seconds

# dimensionality of each subspace
dim_x_z: [1, 1, 1]
# subset of neurons
chosen_neurons: [2, 4, 6, 8, 9, 11, 12, 14, 15, 16, 25, 27, 30, 33]

# 'vae' or 'vae_gp'
# vae has x and z modelled together but independent across time i.e. no GP prior
# vae_gp has x and z modelled separately but with ability to add GP prior on z
which_vae: 'vae_gp'

# parameters for the baseline vae
vae:
  rnn:  
    bidirectional: True
    hidden_size: 8
    num_layers: 3
    dropout: 0.1
    softmax_temp: 1    
  
  lr: 0.01
  weight_decay: 0.01

  scheduler:
    which: ''
    cosine_restart_after: 120
    const_factor: 0.97

vae_gp:
  
  cov_type: 'diagonal' # full, banded or diagonal for z
  apply_softplus: False # whether to apply softplus on the diagonal of the covariance matrix
  z_entropy: # whether to add entropy term to the loss
  softmax_temp: 1 # temperature for the softmax
  
  # stage 1 is initializing 
  load_stage1: #post_stage1_priorx_null2
  freeze_encoder_meanz: False
  load_stage2: False

  # load_stage1: post_stage1_nopriorx
  # freeze_encoder_meanz: True  
  # load_stage2: False

  # load_stage1: 
  # freeze_encoder_meanz: False
  # load_stage2: True

  # gru parameters
  rnn_encoder:
    bidirectional: True
    hidden_size: 8
    num_layers: 2
    dropout: 0.1
  
  # mapping from rnn output to mean and std
  post_rnn_linear:
    hidden_dims: [8]
    dropout: 0

  # whether to add loss terms for disentanglement
  disentangle: True
  
  # kl divergence terms
  kl_beta: 3
  
  # parameters for gp smoothing
  smoothing_sigma: 3
  noise_sigma: 0.01
  kernel_scale: 0.5

  weight_decay: 0.001

  lr: 0.01

  # whether to add monotonicty constraint
  monotonic:
    use: True
    # controls steepness of curve
    nu_z: 1
    nu_g: 1
    coeff: 10
    # which z latents to apply to
    mask: [False, True, False]

# parameters for the supervision
decoder:  
  # which latents to use
  stimulus_latent: 0
  choice_latent: 1
  amplitude_latent:
  
  # weights for the loss terms
  stimulus_weight: 20
  choice_weight: 5
  amplitude_weight:
  
  # train after certain epochs
  train_decoder_after: 0  

  # which
  which: 'cnn' # 'cnn', 'rnn' or 'linear'
  
  cnn_global:    
    channels: [8]
    kernel_size: 3
    dropout: 0
    lr: 0.01
    weight_decay: 0
    normalize_trial_time: True # whether to normalize trial across time
    which_forward: 'argmax' # which forward function to use

  cnn_local:
    channels: [8]
    kernel_size: 3
    dropout: 0
    lr: 0.01
    weight_decay: 0
    normalize_trial_time: True # whether to normalize trial across time

  rnn:
    layers: 1
    hidden_dim: 8
    lr: 0.001
    dropout: 0.3  

  linear:
    hidden_dims: [8, 8]
    dropout: 0.1
    lr: 0.0003

  scheduler:
    which: ''
    cosine_restart_after: 120
    const_factor: 0.99

seed: 7

early_stop:
  patience: 300 # stop if no improvement after this many epochs
  delta: 1 # improvement is defined as a decrease of at least this much
  test_every_new: 2 # how often to test while in slowdown

epochs: 2000
test_every: 20

batch_size: 192 # batch size for training
optim_size: 192 # batch size for optimization step

num_samples_train: 20 # number of variational samples for training
