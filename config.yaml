dir:
  # results: 'results_final/'
  results: 'results_all6/'
  dataset: '../data_own/'

shape_dataset:
  id: sub-221CR_ses-20190515T160400 # my
  # id: sub-221CR_ses-20190514T142312
  # id: sub-231CR_ses-20190924T161413 # abhishek
  # id: sub-231CR_ses-20190921T144923
  # id: sub-KM131_ses-20180117T194023
  # id: sub-267CR_ses-20200331T161053
  # id: sub-245CR_ses-20200331T142427
  # id: sub-KF134_ses-20180208T171905

  win_len: 0.1 # in seconds

# dimensionality of each subspace
dim_x_z: [1, 1, 1]
# subset of neurons
chosen_neurons: [2, 4, 6, 8, 9, 11, 12, 14, 15, 16, 25, 27, 30, 33]

# 'vae' or 'vae_gp'
# vae has x and z modelled together but independent across time i.e. no GP prior
# vae_gp has x and z modelled separately but with ability to add GP prior on z
which_vae: 'vae_gp'

# parameters for the baseline vae
vae:
  rnn:  
    bidirectional: True
    hidden_size: 8
    num_layers: 3
    dropout: 0.1
    softmax_temp: 1    
  
  lr: 0.01
  weight_decay: 0.01

  scheduler:
    which: ''
    cosine_restart_after: 120
    const_factor: 0.97

vae_gp:
  
  cov_type: 'diagonal' # full, banded or diagonal for z
  apply_softplus: False # whether to apply softplus on the diagonal of the covariance matrix
  z_entropy: # whether to add entropy term to the loss
  softmax_temp: 1 # temperature for the softmax
  
  load_stage1: #post_stage1_priorx_null2
  freeze_encoder_meanz: False
  load_stage2: False

  # load_stage1: post_stage1_nopriorx
  # freeze_encoder_meanz: True  
  # load_stage2: False

  # load_stage1: 
  # freeze_encoder_meanz: False
  # load_stage2: True

  # gru parameters
  rnn_encoder:
    bidirectional: True
    hidden_size: 8
    num_layers: 2
    dropout: 0.1
  
  # mapping from rnn output to mean and std
  post_rnn_linear:
    hidden_dims: [8]
    dropout: 0

  # whether to add loss terms for disentanglement
  disentangle: True
  
  # kl divergence terms
  kl_beta: 3
  
  # parameters for gp smoothing
  smoothing_sigma: 3
  noise_sigma: 0.01
  kernel_scale: 0.5

  weight_decay: 0.001

  lr: 0.01

  # whether to add monotonicty constraint
  monotonic:
    use: True
    # controls steepness of curve
    nu_z: 1
    nu_g: 1
    coeff: 10
    # which z latents to apply to
    mask: [False, True, False]

# parameters for the supervision
decoder:
  behavior_weight: 1
  
  # which latents to use
  stimulus_latent: 0
  choice_latent: 1
  amplitude_latent:

  stimulus_weight: 20
  choice_weight: 5
  amplitude_weight:
  
  which: 'cnn_indi'
  # which: 'logreg'
  # which: ''
  
  # train after certain epochs
  train_decoder_after: 0
  # whether to use cross terms
  cross_terms: False  
  
  cnn:
    # channels: [4, 4, 4, 4, 4, 4]
    channels: [8]
    kernel_size: 3
    amp_channels: [8]
    amp_kernel_size: 5
    dropout: 0
    lr: 0.01
    weight_decay: 0
    normalize_trial_time: True
    which_forward: 'argmax'

  rnn:
    layers: 1
    hidden_dim: 8
    lr: 0.001
    dropout: 0.3  

  linear:
    hidden_dims: [8, 8]
    dropout: 0.1
    lr: 0.0003

  scheduler:
    which: ''
    cosine_restart_after: 120
    const_factor: 0.99

seed: 7

early_stop:
  patience: 300
  delta: 1
  test_every_new: 2

epochs: 2000
test_every: 20
plot_every: 5

batch_size: 192
optim_size: 192

num_samples_train: 20

z_prior:
  include: False
  weights: [100, 0]
  # means and stds multipled by number of bins  
  means: [0, 1]
  stds: [0.2, 1]
  learn: True
  prior_on_mean: False