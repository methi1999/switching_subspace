dir:
  results: 'results/'
  dataset: '../data_own/'

shape_dataset:
  id: sub-221CR_ses-20190515T160400 # my
  # id: sub-221CR_ses-20190514T142312
  # id: sub-231CR_ses-20190924T161413 # abhishek
  # id: sub-231CR_ses-20190921T144923
  # id: sub-KM131_ses-20180117T194023
  # id: sub-267CR_ses-20200331T161053
  # id: sub-245CR_ses-20200331T142427
  # id: sub-KF134_ses-20180208T171905

  win_len: 0.1

# mention x for stimulus, choice and rest
dim_x_z: [1, 1, 1]
# dim_x_z: [2, 2, 2]
# dim_x_z: [0, 0, 3]
chosen_neurons: [2, 4, 6, 8, 9, 11, 12, 14, 15, 16, 25, 27, 30, 33]

which_vae: 'vae_gp_combined' # baseline or vae_gp
# which_vae: 'baseline' # baseline or vae_gp

vae_baseline:
  rnn:  
    bidirectional: True
    hidden_size: 8
    num_layers: 3
    dropout: 0.1
    softmax_temp: 1    
  
  # smoothing: 0.1
  lr: 0.01
  weight_decay: 0.01

  scheduler:
    which: ''
    cosine_restart_after: 120
    const_factor: 0.97

vae_gp:
  
  cov_type: 'diagonal' # full, banded or diagonal for z
  apply_softplus: False
  z_entropy:
  softmax_temp: 1  
  # load_pt: vae_gp_2_8_weights_[2, 2, 2]_dynamic.pth
  load_pt: vae_gp_2_8_weights_[1, 1, 1]_dynamic.pth
  
  rnn_encoder:
    bidirectional: True
    hidden_size: 8
    num_layers: 2
    dropout: 0.2
  
  post_rnn_linear:
    hidden_dims: [8]
    dropout: 0

  kl_beta: 3
  
  smoothing_sigma: 3
  noise_sigma: 0.01
  rbf_scaling: 0.5

  weight_decay: 0.001

  lr: 0.01

  monotonic:
    use: True
    nu_z: 1
    nu_g: 1
    coeff: 10
    mask: [True, True, False]

decoder:
  behavior_weight: 1
  
  stimulus_latent: 0
  choice_latent: 1
  amplitude_latent:

  stimulus_weight: 20
  choice_weight: 10
  amplitude_weight:
  
  which: 'cnn_indi'
  # which: ''
  train_decoder_after: 0
  cross_terms: False
  
  cnn:
    # channels: [4, 4, 4, 4, 4, 4]
    channels: [8]
    kernel_size: 5
    amp_channels: [8]
    amp_kernel_size: 5
    dropout: 0
    lr: 0.01
    weight_decay: 0
    normalize_trial_time: True

  rnn:
    layers: 1
    hidden_dim: 8
    lr: 0.001
    dropout: 0.3  

  linear:
    hidden_dims: [8, 8]
    dropout: 0.1
    lr: 0.0003

  scheduler:
    which: ''
    cosine_restart_after: 120
    const_factor: 0.99

seed: 7

early_stop:
  patience: 300
  delta: 1
  test_every_new: 2

epochs: 2000
test_every: 20
plot_every: 5

batch_size: 192
optim_size: 192

num_samples_train: 20

z_prior:
  include: False
  weights: [100, 0]
  # means and stds multipled by number of bins  
  means: [0, 1]
  stds: [0.2, 1]
  learn: True
  prior_on_mean: False