{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "from model import Model\n",
    "import utils\n",
    "from early_stopping import EarlyStopping\n",
    "from priors import moving_average\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = utils.read_config()\n",
    "# set seeds\n",
    "utils.set_seeds(config['seed'])\n",
    "# utils.set_seeds(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviour_data, spikes = utils.load_dataset(config)\n",
    "# consider data from only t = -1\n",
    "# time_from = int(1/bin_len)\n",
    "# behaviour_data, spikes = [x[time_from:, :] for x in behaviour_data], [x[time_from:, :] for x in spikes]\n",
    "num_trials, time_bins, emissions_dim = np.array(spikes).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_idx, choice_idx = 6, 3\n",
    "stim = [x[0, stim_idx] for x in behaviour_data]\n",
    "choice = [x[0, choice_idx] for x in behaviour_data]\n",
    "num_contacts = [np.sum(x[:, -9:-5], axis=1) for x in behaviour_data]\n",
    "# concat them\n",
    "behaviour_data = np.stack((stim, choice), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to torch tensors\n",
    "behaviour_data = torch.tensor(behaviour_data, dtype=torch.long)\n",
    "# behaviour_data = torch.tensor(behaviour_data, dtype=torch.float32)\n",
    "spikes = torch.tensor(spikes, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader with random sampling for training and testing\n",
    "# split data into training and testing\n",
    "behaviour_data_train, behaviour_data_test, spikes_train, spikes_test = train_test_split(behaviour_data, spikes, test_size=0.3, random_state=42)\n",
    "# behaviour_data_train, behaviour_data_test, spikes_train, spikes_test = train_test_split(behaviour_data, spikes, test_size=0.2, random_state=7)\n",
    "\n",
    "# create dataloaders\n",
    "train_dataset = TensorDataset(behaviour_data_train, spikes_train)\n",
    "test_dataset = TensorDataset(behaviour_data_test, spikes_test)\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train distribution of Stimulus: 0.5089285714285714, Choice: 0.4375\n",
      "Test distribution of Stimulus: 0.42857142857142855, Choice: 0.2653061224489796\n"
     ]
    }
   ],
   "source": [
    "# distribution of choice and stimulus in test\n",
    "print(\"Train distribution of Stimulus: {}, Choice: {}\".format(np.mean(behaviour_data_train[:, 0].numpy()), np.mean(behaviour_data_train[:, 1].numpy())))\n",
    "print(\"Test distribution of Stimulus: {}, Choice: {}\".format(np.mean(behaviour_data_test[:, 0].numpy()), np.mean(behaviour_data_test[:, 1].numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean firing rate of neurons in tran spikes\n",
    "neuron_bias = torch.mean(spikes_train, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if mps is available\n",
    "# device = torch.device('mps' if torch.backends.mps.is_built() else 'cpu')\n",
    "# print(device)\n",
    "# model = model.to(device)\n",
    "# spikes = spikes.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, n_samples=200):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for _, (behavior_batch, spikes_batch) in enumerate(test_loader):\n",
    "            y_recon, mu, A, z, x, behavior = model.forward(spikes_batch, n_samples=n_samples)\n",
    "            _, loss_l = model.loss(None, spikes_batch, y_recon, mu, A, z, x, behavior, behavior_batch)\n",
    "            # l.append(loss_l[1])\n",
    "            test_loss += np.array(loss_l)            \n",
    "            # print(np.mean(l), np.std(l))\n",
    "    # divide loss by total number of samples in dataloader    \n",
    "    return test_loss/len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using moving average\n",
      "Scheduler not implemented for GRU\n",
      "Number of trainable parameters in VAE: 5888\n",
      "No behavior decoder\n"
     ]
    }
   ],
   "source": [
    "config = utils.read_config()\n",
    "# training loop\n",
    "num_epochs = config['epochs']\n",
    "# create model and optimizer\n",
    "model = Model(config, input_dim=emissions_dim) #, neuron_bias=neuron_bias\n",
    "# model = torch.compile(model)\n",
    "early_stop = EarlyStopping(patience=config['early_stop']['patience'], delta=config['early_stop']['delta'], trace_func=print)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', threshold=1, verbose=True, patience=5, factor=0.5)\n",
    "# print named parameters of model\n",
    "# print(\"Model's state_dict:\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/2000], Train Loss: [570.05297852], Test Loss: [587.59106445], Best Loss: 587.591064453125\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "def train(model: Model, train_loader, val_loader):    \n",
    "    test_every = config['test_every']    \n",
    "    train_decoder_after = config['decoder']['train_decoder_after']    \n",
    "    num_samples_train = config['num_samples_train']\n",
    "    save_model = True    \n",
    "    for epoch in range(num_epochs):\n",
    "        # forward pass\n",
    "        # print(model.behavior_decoder.scheduler.get_last_lr())\n",
    "        # model.vae.scheduler.get_last_lr()\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        for i, (behavior_batch, spikes_batch) in enumerate(train_loader):            \n",
    "            # behavior_batch = behavior_batch.long()\n",
    "            y_recon, mu, A, z, x, behavior_pred = model(spikes_batch, n_samples=num_samples_train)\n",
    "            # calculate loss\n",
    "            loss, loss_l = model.loss(epoch, spikes_batch, y_recon, mu, A, z, x, behavior_pred, behavior_batch)            \n",
    "            # backward pass\n",
    "            model.optim_zero_grad()\n",
    "            loss.backward()\n",
    "            # print gradient of any weight\n",
    "            # if epoch > 10:\n",
    "            #     print(model.behavior_decoder.conv_choice[1].weight.grad)            \n",
    "            model.optim_step(train_decoder = epoch >= train_decoder_after)                \n",
    "            epoch_loss += np.array(loss_l)\n",
    "        \n",
    "        # if epoch % 100 == 0:\n",
    "        #     # print lr of decoder\n",
    "        #     print(model.behavior_decoder.scheduler.get_last_lr())\n",
    "        train_losses.append((epoch, epoch_loss/len(train_loader)))\n",
    "        model.scheduler_step(step_decoder = epoch >= train_decoder_after)\n",
    "        # test loss\n",
    "        if (epoch+1) % test_every == 0:            \n",
    "            test_loss = test(model, val_loader)\n",
    "            sum_test_loss = np.sum(test_loss)\n",
    "            # scheduler.step(sum_test_loss)\n",
    "            test_losses.append((epoch, test_loss))\n",
    "            early_stop(sum_test_loss, model, save_model=save_model, save_prefix='best')\n",
    "            model.save_model(save_prefix=str(epoch))\n",
    "            print('Epoch [{}/{}], Train Loss: {}, Test Loss: {}, Best Loss: {}'.format(epoch+1, num_epochs, train_losses[-1][1], test_losses[-1][1], early_stop.best_score))\n",
    "            if early_stop.slow_down:\n",
    "                test_every = config['early_stop']['test_every_new']\n",
    "            else:\n",
    "                test_every = config['test_every']\n",
    "            if early_stop.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "            \n",
    "    \n",
    "    only_test_loss = [np.sum(x[1]) for x in test_losses]\n",
    "    \n",
    "    # compute min test loss and return it    \n",
    "    # return np.min(only_test_loss), train_losses, test_losses\n",
    "    \n",
    "    # compute median of test loss in a window of 5\n",
    "    meds = []\n",
    "    half_window = 10\n",
    "    only_test_loss = [0]*(half_window) + only_test_loss + [0]*(half_window)\n",
    "    for i in range(half_window, len(only_test_loss)-half_window):\n",
    "        meds.append(np.max(only_test_loss[i-half_window:i+half_window]))\n",
    "    return np.min(meds), train_losses, test_losses\n",
    "\n",
    "_ = train(model, train_loader, test_loader)\n",
    "# train model\n",
    "# min_test_loss, train_losses, test_losses = train(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses_og, test_losses_og = train_losses[:], test_losses[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.prior_modules[0].plot_gaussian()\n",
    "# model.prior_modules[0].log_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_curve(model, config, train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort list by test loss\n",
    "sorted_loss = sorted(test_losses, key=lambda x: x[1][1], reverse=True) \n",
    "# extract first element after epoch 600\n",
    "sorted_loss = [(x[0], x[1][1]/100) for x in sorted_loss if x[0] > 100]\n",
    "print(sorted_loss[:2], sorted_loss[-10:])\n",
    "# print(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "model.load_model('best')\n",
    "# load model from epoch x\n",
    "# model.load_model('159')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lin_maps = model.vae.linear_maps\n",
    "# # c1, c2 = lin_maps[0].weight.detach().numpy(), lin_maps[1].weight.detach().numpy()\n",
    "# # print(c1.T.dot(c2)/(np.linalg.norm(c1)*np.linalg.norm(c2)))\n",
    "# c1, c2, c3 = lin_maps[0].weight.detach().numpy(), lin_maps[1].weight.detach().numpy(), lin_maps[2].weight.detach().numpy()\n",
    "# print(\"Norms: {}, {}, {}\".format(np.linalg.norm(c1), np.linalg.norm(c2), np.linalg.norm(c3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    y_recon, mu, A, _, _, _ = model.forward(spikes, n_samples=1)\n",
    "    # run on only test\n",
    "    y_recon_test, mu_test, A_test, _, _, _ = model.forward(spikes_test, n_samples=1)\n",
    "    \n",
    "# convert to numpy\n",
    "y_recon_np = y_recon.detach().numpy()\n",
    "spikes_np = spikes.detach().numpy()\n",
    "y_recon_test_np = y_recon_test.detach().numpy()\n",
    "spikes_test_np = spikes_test.detach().numpy()\n",
    "# compute bits/spike\n",
    "bits_per_spike_all = utils.bits_per_spike(y_recon_np, spikes_np)\n",
    "bits_per_spike_test = utils.bits_per_spike(y_recon_test_np, spikes_test_np)\n",
    "# show distribution of bits per spike\n",
    "plt.hist(bits_per_spike_all, bins=50)\n",
    "plt.xlabel('Bits/spike')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "# print('Bits per spike: {}'.format(bits_per_spike))\n",
    "print(\"Bits per spike all: {}, test: {}\".format(np.sum(bits_per_spike_all), np.sum(bits_per_spike_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_t = np.mean(A.numpy()[0:1, 20, :, :], axis=0)\n",
    "cov = a_t * a_t.T\n",
    "# print(cov.shape, spikes_np.shape)\n",
    "plt.imshow(cov)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PSTH of reconstructed and original data\n",
    "averaged_recon, averaged_original = y_recon.mean(axis=0), spikes_np.mean(axis=0)\n",
    "# stimulus and choice important\n",
    "common = [12, 14, 4, 31]\n",
    "stim_neurons = [15, 11, 33, 30]\n",
    "choice_neurons = [16, 2, 6, 8]\n",
    "# plot each in a 5x7 grid\n",
    "fig, axs = plt.subplots(5, 7, figsize=(12, 9))\n",
    "# set title of figure\n",
    "fig.suptitle('yellow: choice, green: stimulus, pink: common')\n",
    "for i in range(5):\n",
    "    for j in range(7):\n",
    "        neuron_idx = i*7+j        \n",
    "        axs[i, j].plot(averaged_recon[:, neuron_idx], label='recon', color='red')\n",
    "        axs[i, j].plot(averaged_original[:, neuron_idx], label='original', color='blue')\n",
    "        # no ticks\n",
    "        axs[i, j].set_xticks([])\n",
    "        axs[i, j].set_yticks([])\n",
    "        # set title of plot to neuron index\n",
    "        # print only 2 decimal places        \n",
    "        axs[i, j].set_title('{}: {:.4f}'.format(neuron_idx, bits_per_spike_all[neuron_idx]))\n",
    "        # set background color of plot to green if neuron in choice\n",
    "        if neuron_idx in choice_neurons:\n",
    "            axs[i, j].set_facecolor('yellow')\n",
    "        # set background color of plot to red if neuron in stimulus\n",
    "        if neuron_idx in stim_neurons:\n",
    "            axs[i, j].set_facecolor('green')\n",
    "        # set background color of plot to blue if neuron in common\n",
    "        if neuron_idx in common:\n",
    "            axs[i, j].set_facecolor('pink')\n",
    "axs[0, 0].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot trial averaged latent space\n",
    "# with torch.no_grad():\n",
    "#     _, (mu, A), (z, x), _ = model.forward(spikes)\n",
    "#     z, x = z.numpy(), x.numpy()\n",
    "# # z, x = torch.nn.Softmax(dim=2)(mu[:, :, :model.vae.z_dim]).numpy(), mu[:, :, model.vae.z_dim:].numpy()\n",
    "# # z_std, x_std = np.std(z, axis=0), np.std(x, axis=0)\n",
    "# fig, axs = plt.subplots(2, 1, figsize=(4, 8))\n",
    "# z_avg, x_avg = np.mean(z, axis=0), np.mean(x, axis=0)\n",
    "# # make x ticks of range 0.1 from -2 to 0.5\n",
    "# bin_len = config['shape_dataset']['win_len']\n",
    "# t = np.arange(-2, 0.5, bin_len)\n",
    "# for i in range(z.shape[2]):\n",
    "#     axs[0].plot(t, z_avg[:, i], label='z{}'.format(i))\n",
    "#     axs[1].plot(t, x_avg[:, i], label='x{}'.format(i))\n",
    "# axs[0].set_title('z')\n",
    "# axs[1].set_title('x')\n",
    "# axs[0].legend()\n",
    "# axs[1].legend()\n",
    "\n",
    "# plot trial averaged latent space\n",
    "z, x = torch.nn.Softmax(dim=2)(mu[:, :, :model.vae.z_dim]).numpy(), mu[:, :, model.vae.z_dim:].numpy()\n",
    "z_std, x_std = np.std(z, axis=0), np.std(x, axis=0)\n",
    "z_avg, x_avg = np.mean(z, axis=0), np.mean(x, axis=0)\n",
    "# make x ticks of range 0.1 from -2 to 0.5\n",
    "bin_len = config['shape_dataset']['win_len']\n",
    "t = np.arange(-2, 0.5, bin_len)\n",
    "for i in range(z.shape[2]):\n",
    "    plt.plot(t, z_avg[:, i], label='z{}'.format(i))    \n",
    "    plt.fill_between(t, z_avg[:, i]-z_std[:, i], z_avg[:, i]+z_std[:, i], alpha=0.3)\n",
    "# plt.set_title('z')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.arch_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cont_latent(data, label, ax_id, linestyle):\n",
    "    num_latents = data.shape[1]\n",
    "    for i in range(num_latents):\n",
    "        axs[ax_id].plot(t, data[:, i], label='{}_x{}'.format(label, i),\n",
    "                        color=colors[i], linestyle=linestyle)    \n",
    "        \n",
    "colors = ['red', 'blue', 'green', 'yellow', 'black', 'pink']\n",
    "# group x for stimulus and choice\n",
    "stim, choice = behaviour_data[:, 0].numpy(), behaviour_data[:, 1].numpy()\n",
    "# group x for stimulus\n",
    "x_stim_left, x_stim_right = x[stim == 1].mean(axis=0), x[stim == 0].mean(axis=0)\n",
    "x_choice_left, x_choice_right = x[choice == 1].mean(axis=0), x[choice == 0].mean(axis=0)\n",
    "# plot x for stimulus and choice\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "plot_cont_latent(x_stim_left, 'stim left', 0, '-')\n",
    "plot_cont_latent(x_stim_right, 'stim right', 0, '-.')\n",
    "axs[0].set_title('x grouped by stimulus')\n",
    "axs[0].legend()\n",
    "\n",
    "plot_cont_latent(x_choice_left, 'choice left', 1, '-')\n",
    "plot_cont_latent(x_choice_right, 'choice right', 1, '-.')\n",
    "axs[1].set_title('x grouped by choice')\n",
    "axs[1].legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot x on 2d\n",
    "choice = behaviour_data[:, 1].numpy()\n",
    "# group x for stimulus\n",
    "x_choice_left, x_choice_right = x[choice == 1].mean(axis=0), x[choice == 0].mean(axis=0)\n",
    "z_choice_left, z_choice_right = z[choice == 1].mean(axis=0), z[choice == 0].mean(axis=0)\n",
    "# plot first 2 dimensions\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.plot(x_choice_left[:, 2], x_choice_left[:, 3], label='choice left', color='red')\n",
    "ax.plot(x_choice_right[:, 2], x_choice_right[:, 3], label='choice right', color='blue')\n",
    "ax.set_title('x grouped by choice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decoder\n",
    "with torch.no_grad():\n",
    "    model.eval()    \n",
    "    _, (mu_train, A_train), (z_train, x_train), behavior_pred_train = model.forward(spikes_train)\n",
    "    _, (mu_test, A_test), (z_test, x_test), behavior_pred_test = model.forward(spikes_test)\n",
    "# if behavior_pred_train is None:\n",
    "# train the linear decoder for behavior\n",
    "# create linear decoder\n",
    "linear_decoder = decoder.CNNDecoder(config, input_dim=2)\n",
    "optimizer = torch.optim.Adam(linear_decoder.parameters(), lr=0.01)\n",
    "decoder_train_l, decoder_test_l = [], []\n",
    "for epoch in range(100):\n",
    "    linear_decoder.train()\n",
    "    # forward pass        \n",
    "    linear_decoder.train()\n",
    "    behavior_pred = linear_decoder(x_train, z_train)\n",
    "    # behavior_pred = linear_decoder(mu_train[:, :, :2], mu_train[:, :, 2:])\n",
    "    loss = linear_decoder.loss(behavior_pred, behaviour_data_train, None)\n",
    "    # backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()    \n",
    "    epoch_loss = loss.item()    \n",
    "    decoder_train_l.append(epoch_loss)\n",
    "    # test loss\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        linear_decoder.eval()\n",
    "        test_pred = linear_decoder(x_test, z_test)\n",
    "        # behavior_pred = linear_decoder(mu_train[:, :, :2], mu_train[:, :, 2:])\n",
    "        test_loss = linear_decoder.loss(test_pred, behaviour_data_test, None).item()        \n",
    "        decoder_test_l.append(test_loss)\n",
    "        print('Epoch [{}/{}], Train Loss: {:.4f}, Test Loss: {:.4f}'.format(epoch+1, num_epochs, decoder_train_l[-1], decoder_test_l[-1]))\n",
    "    # if epoch % 10 == 0:\n",
    "    #     torch.save(model.state_dict(), os.path.join(base_path, 'vae_model_{}.pt'.format(epoch)))\n",
    "    #     print('Model saved at epoch {}'.format(epoch))\n",
    "with torch.no_grad():\n",
    "    behavior_pred_train = linear_decoder(x_train, z_train).detach()\n",
    "    behavior_pred_test = linear_decoder(x_test, z_test).detach()\n",
    "    # behavior_pred_train = linear_decoder(mu_train[:, :, :2], mu_train[:, :, 2:])\n",
    "    # behavior_pred_test = linear_decoder(mu_test[:, :, :2], mu_test[:, :, 2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100\n",
    "agg_pred_train, agg_pred_test = [], []\n",
    "agg_y_train, agg_y_test = [], []\n",
    "\n",
    "# convert to numpy\n",
    "y_train = behaviour_data_train.numpy()\n",
    "y_test = behaviour_data_test.numpy()\n",
    "# accuracy of stimulus and choice\n",
    "acc_stim_train, acc_stim_test = [], []\n",
    "acc_choice_train, acc_choice_test = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    _, _, _, _, _, behavior_pred_train_all = model.forward(spikes_train, n_samples=num_samples)\n",
    "    _, _, _, _, _, behavior_pred_test_all = model.forward(spikes_test, n_samples=num_samples)        \n",
    "    for k in range(num_samples):\n",
    "        b1 = behavior_pred_train_all.shape[0]//num_samples\n",
    "        b2 = behavior_pred_test_all.shape[0]//num_samples\n",
    "        behavior_pred_train = behavior_pred_train_all[k*b1: k*b1+b1]\n",
    "        behavior_pred_test = behavior_pred_test_all[k*b2: k*b2+b2]\n",
    "        # print(behavior_pred_test.shape, y_test.shape)\n",
    "        # convert to numpy\n",
    "        # print(behavior_pred_train[20:30, 1])\n",
    "        # pred_train = behavior_pred_train.numpy() > 0\n",
    "        # pred_test = behavior_pred_test.numpy() > 0        \n",
    "        pred_train_stim = torch.argmax(behavior_pred_train[:, :2], dim=1).numpy()\n",
    "        pred_test_stim = torch.argmax(behavior_pred_test[:, :2], dim=1).numpy()\n",
    "        pred_train_choice = torch.argmax(behavior_pred_train[:, 2:], dim=1).numpy()\n",
    "        pred_test_choice = torch.argmax(behavior_pred_test[:, 2:], dim=1).numpy()\n",
    "        pred_train = np.concatenate((pred_train_stim[:, None], pred_train_choice[:, None]), axis=1)\n",
    "        pred_test = np.concatenate((pred_test_stim[:, None], pred_test_choice[:, None]), axis=1)        \n",
    "        # pred_test = behavior_pred_test.numpy() > 0\n",
    "        # compute accuracy        \n",
    "        accuracy_train_stim = accuracy_score(y_train[:, 0], pred_train[:, 0])\n",
    "        accuracy_test_stim = accuracy_score(y_test[:, 0], pred_test[:, 0])        \n",
    "        # do the same for choice\n",
    "        accuracy_train_choice = accuracy_score(y_train[:, 1], pred_train[:, 1])\n",
    "        accuracy_test_choice = accuracy_score(y_test[:, 1], pred_test[:, 1])\n",
    "        # append to list\n",
    "        acc_stim_train.append(accuracy_train_stim)\n",
    "        acc_stim_test.append(accuracy_test_stim)\n",
    "        acc_choice_train.append(accuracy_train_choice)\n",
    "        acc_choice_test.append(accuracy_test_choice)\n",
    "# print accuracy\n",
    "print('Stimulus Accuracy - train: {:.4f} +- {:.4f}, test: {:.4f} +- {:.4f}'.format(np.mean(acc_stim_train), np.std(acc_stim_train), np.mean(acc_stim_test), np.std(acc_stim_test)))\n",
    "print('Choice Accuracy - train: {:.4f} +- {:.4f}, test: {:.4f} +- {:.4f}'.format(np.mean(acc_choice_train), np.std(acc_choice_train), np.mean(acc_choice_test), np.std(acc_choice_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine a random trial\n",
    "trial_idx = np.random.randint(num_trials)\n",
    "# trial_idx = 20\n",
    "# plot z and x\n",
    "fig, axs = plt.subplots(3, 1, figsize=(5, 8))\n",
    "# plot z\n",
    "bin_len = config['shape_dataset']['win_len']\n",
    "t = np.arange(-2, 0.5, bin_len)\n",
    "axs[0].plot(t, z[trial_idx, :, 0], label='z0')\n",
    "axs[0].plot(t, z[trial_idx, :, 1], label='z1')\n",
    "# axs[0].plot(t, z[trial_idx, :, 2], label='z2')\n",
    "axs[0].set_title('Mean of z for a random trial')\n",
    "axs[0].set_ylim(0, 1)\n",
    "axs[0].legend()\n",
    "axs[0].set_xticks([])\n",
    "# plot num contacts\n",
    "axs[1].plot(t, num_contacts[trial_idx])\n",
    "axs[1].set_title('num contacts')\n",
    "axs[1].set_xticks([])\n",
    "# plot x\n",
    "axs[2].plot(t, x[trial_idx, :, 0], label='x0')\n",
    "axs[2].plot(t, x[trial_idx, :, 1], label='x1')\n",
    "# axs[2].plot(t, x[trial_idx, :, 2], label='x2')\n",
    "axs[2].set_title('x, stimulus: {}, choice: {}'.format(stim[trial_idx].astype(int), choice[trial_idx].astype(int)))\n",
    "axs[2].set_ylim(-2, 2)\n",
    "axs[2].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine a random trial\n",
    "# trial_idx = np.random.randint(len(spikes_test))\n",
    "trial_idx = 20\n",
    "# plot z and x\n",
    "fig, axs = plt.subplots(3, 1, figsize=(5, 8))\n",
    "# plot z\n",
    "bin_len = config['shape_dataset']['win_len']\n",
    "t = np.arange(-2, 0.5, bin_len)\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    _, (mu_train, A_train), (z_sample, x_sample), behavior_pred_train = model.forward(spikes[trial_idx: trial_idx+1])\n",
    "z_sample, x_sample = z_sample[0].numpy(), x_sample[0].numpy()\n",
    "axs[0].plot(t, z_sample[:, 0], label='z0')\n",
    "axs[0].plot(t, z_sample[:, 1], label='z1')\n",
    "# axs[0].plot(t, z[trial_idx, :, 2], label='z2')\n",
    "axs[0].set_title('Sample of z for a random trial')\n",
    "axs[0].set_ylim(0, 1)\n",
    "axs[0].legend()\n",
    "axs[0].set_xticks([])\n",
    "# plot num contacts\n",
    "axs[1].plot(t, num_contacts[trial_idx])\n",
    "axs[1].set_title('num contacts')\n",
    "axs[1].set_xticks([])\n",
    "# plot x\n",
    "axs[2].plot(t, x_sample[:, 0], label='x0')\n",
    "axs[2].plot(t, x_sample[:, 1], label='x1')\n",
    "# axs[2].plot(t, x[trial_idx, :, 2], label='x2')\n",
    "axs[2].set_title('x, stimulus: {}, choice: {}'.format(stim[trial_idx].astype(int), choice[trial_idx].astype(int)))\n",
    "axs[2].set_ylim(-2, 2)\n",
    "axs[2].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a sklearn logistic regression model for comparison\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def reshape(x, y):\n",
    "    trials, time, dim = x.shape\n",
    "    # x = x[:, -5:-4, :]\n",
    "    return x.reshape(trials, -1), y\n",
    "\n",
    "stim_choice = 1\n",
    "\n",
    "x_train_baseline, y_train_baseline = reshape(spikes_train, behaviour_data_train)\n",
    "x_test_baseline, y_test_baseline = reshape(spikes_test, behaviour_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train logistic regression model\n",
    "log_reg = LogisticRegression(penalty='l1', C=1, solver='liblinear', verbose=1)\n",
    "# log_reg = SVC()\n",
    "log_reg.fit(x_train_baseline, y_train_baseline[:, stim_choice])\n",
    "# test accuracy\n",
    "accuracy_train = accuracy_score(y_train_baseline[:, stim_choice], log_reg.predict(x_train_baseline))\n",
    "accuracy_test = accuracy_score(y_test_baseline[:, stim_choice], log_reg.predict(x_test_baseline))\n",
    "print('Logistic Regression Accuracy - train: {:.4f}, test: {:.4f}'.format(accuracy_train, accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# Define logistic regression model\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(875, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = LogisticRegressionModel()\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss for binary classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2000\n",
    "train_losses, test_losses = [], []\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(x_train_baseline)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = criterion(outputs, y_train_baseline[:, stim_choice:stim_choice+1])\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    # Print the loss every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Test the model\n",
    "    with torch.no_grad():    \n",
    "        predicted_prob = model(x_test_baseline)\n",
    "        test_losses.append(criterion(predicted_prob, y_test_baseline[:, stim_choice:stim_choice+1]).item())\n",
    "        predicted_class = (predicted_prob >= 0.5).float()\n",
    "        accuracy = (predicted_class == y_test_baseline[:, stim_choice:stim_choice+1]).float().mean()\n",
    "print(f'Accuracy: {accuracy.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(test_losses, label='test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log reg coeff\n",
    "# plot coefficients in a 2d grid\n",
    "# c = log_reg.coef_.reshape(-1, 25)\n",
    "# plt.imshow(c)\n",
    "\n",
    "# plot nn coefficients\n",
    "c = model.linear.weight.detach().numpy().reshape(-1, 25)\n",
    "plt.imshow(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean and standard deviation of decoding loss over trials\n",
    "model.eval()\n",
    "# print(\"Total models to study:\", len(test_losses[:10] + test_losses[10::20]))\n",
    "print(\"Total models to study:\", len(test_losses))\n",
    "test_losses_aggregated = []\n",
    "z_agg, x_agg = [], []\n",
    "cov_norm = []\n",
    "cnn_weights = []\n",
    "decoding_runs = 50\n",
    "# for epoch, _ in test_losses[:10] + test_losses[10::20]:\n",
    "for epoch, _ in test_losses:\n",
    "    # print(epoch)\n",
    "    # if epoch != 331:\n",
    "    #     continue\n",
    "    model.load_model(str(epoch))\n",
    "    cur_epoch_loss = []\n",
    "    cur_epoch_z, cur_epoch_x = [], []\n",
    "    with torch.no_grad():\n",
    "        cov_norm_current = []\n",
    "        for _ in range(decoding_runs):\n",
    "            test_loss = []\n",
    "            cur_run_z, cur_run_x = [], []\n",
    "            for _, (behavior_batch, spikes_batch) in enumerate(test_loader):\n",
    "                y_recon, (mu, A), (z, x), behavior_batch_pred = model(spikes_batch)\n",
    "                behavior_loss = model.behavior_decoder.loss(behavior_batch_pred, behavior_batch, z, reduction='none')                \n",
    "                test_loss.append(behavior_loss)\n",
    "                cur_run_z.append(z[:, :, 0])\n",
    "                cur_run_x.append(x[:, :, 0])\n",
    "                cov_norm_current.append(np.linalg.norm(A.numpy()))\n",
    "            # stack horizontally and take mean across samples\n",
    "            test_loss = torch.mean(torch.cat(test_loss, dim=0), dim=0).item()\n",
    "            cur_epoch_loss.append(test_loss)\n",
    "            # stack z and x\n",
    "            z_stacked, x_stacked = torch.cat(cur_run_z, dim=0), torch.cat(cur_run_x, dim=0)            \n",
    "            cur_epoch_z.append(z_stacked)\n",
    "            cur_epoch_x.append(x_stacked)\n",
    "        \n",
    "        test_losses_aggregated.append((epoch, np.mean(cur_epoch_loss), np.std(cur_epoch_loss)))\n",
    "        # compute std for z and x\n",
    "        z_std, x_std = torch.std(torch.stack(cur_epoch_z), dim=0), torch.std(torch.stack(cur_epoch_x), dim=0)        \n",
    "        \n",
    "        z_agg.append((epoch, z_std.mean().item()))\n",
    "        x_agg.append((epoch, x_std.mean().item()))\n",
    "        # cov norm\n",
    "        cov_norm.append((epoch, np.mean(cov_norm_current)))\n",
    "        # cnn weights\n",
    "        cnn_weights.append((epoch, deepcopy(model.behavior_decoder.conv_stim)))        \n",
    "# # plot histogram\n",
    "# plt.hist(test_loss.numpy(), range=(0, 7), bins=50, density=True)\n",
    "# plt.xlabel('Cross entropy loss')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Test decoding loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mean and std of test loss\n",
    "test_losses_plot = np.array(test_losses_aggregated)\n",
    "# plt.errorbar(test_losses_plot[:, 0], test_losses_plot[:, 1], yerr=test_losses_plot[:, 2])\n",
    "# print(test_losses_plot)\n",
    "plt.plot(test_losses_plot[:, 0], test_losses_plot[:, 1])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cross entropy loss')\n",
    "plt.title('Test decoding loss mean and std over 200 runs at various epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot x and z\n",
    "print(z_agg)\n",
    "z_agg_plot = np.array(z_agg)\n",
    "x_agg_plot = np.array(x_agg)\n",
    "plt.plot(z_agg_plot[:, 0], z_agg_plot[:, 1], label='z')\n",
    "plt.plot(x_agg_plot[:, 0], x_agg_plot[:, 1], label='x')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Standard deviation')\n",
    "plt.title('Standard deviation of z and x over 200 runs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot norm of covariance\n",
    "cov_norm_plot = np.array(cov_norm)\n",
    "plt.plot(cov_norm_plot[:, 0], cov_norm_plot[:, 1])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Norm of covariance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine particular examples\n",
    "# get mean and standard deviation of decoding loss over trials\n",
    "model.eval()\n",
    "print(\"Total models to study:\", len(test_losses))\n",
    "test_losses_aggregated = []\n",
    "decoding_runs = 100\n",
    "for epoch, _ in test_losses[:10] + test_losses[10::20]:\n",
    "    # print(epoch)\n",
    "    model.load_model(str(epoch))\n",
    "    cur_epoch_loss = []    \n",
    "    with torch.no_grad():        \n",
    "        for _ in range(decoding_runs):\n",
    "            test_loss = []\n",
    "            cur_run_z, cur_run_x = [], []\n",
    "            for _, (behavior_batch, spikes_batch) in enumerate(test_loader):\n",
    "                y_recon, (mu, A), (z, x), behavior_batch_pred = model(spikes_batch)\n",
    "                behavior_loss = model.behavior_decoder.loss(behavior_batch_pred, behavior_batch, z, reduction='none')                \n",
    "                test_loss.append(behavior_loss)                \n",
    "            # stack horizontally and take mean across samples\n",
    "            test_loss = torch.cat(test_loss, dim=0)\n",
    "            cur_epoch_loss.append(test_loss)\n",
    "        # stack the losses across decoding runs\n",
    "        cur_epoch_loss = torch.stack(cur_epoch_loss, dim=0)\n",
    "        # compute mean and std of epoch loss across decoding runs\n",
    "        m, s = torch.mean(cur_epoch_loss, dim=0).numpy(), torch.std(cur_epoch_loss, dim=0).numpy()        \n",
    "        test_losses_aggregated.append((epoch, m, s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(15):\n",
    "    epochs = [x[0] for x in test_losses_aggregated]\n",
    "    losses = [x[2][idx] for x in test_losses_aggregated]\n",
    "    plt.plot(epochs, losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cross entropy loss')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot norm of change in CNN weights over time\n",
    "base_cnn = cnn_weights[0][1]\n",
    "layers_to_study = [(x, base_cnn[x].__module__.split('.')[-1]) for x in range(len(base_cnn)) if isinstance(base_cnn[x], torch.nn.Conv1d) or isinstance(base_cnn[x], torch.nn.BatchNorm1d)]\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "for layer_idx, name in layers_to_study:    \n",
    "    change_in_norms = []\n",
    "    for i, (epoch, cur_cnn) in enumerate(cnn_weights):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        cur_weight = cur_cnn[layer_idx].weight.detach().numpy()\n",
    "        prev_epoch_weights = cnn_weights[i-1][1][layer_idx].weight.detach().numpy()        \n",
    "        change_in_norm = np.linalg.norm(cur_weight - prev_epoch_weights)        \n",
    "        # normalize by total number of weights        \n",
    "        change_in_norm /= cur_weight.size\n",
    "        change_in_norms.append((epoch, change_in_norm))\n",
    "    change_in_norms = np.array(change_in_norms)    \n",
    "    ax1.plot(change_in_norms[:, 0], change_in_norms[:, 1], label=name)\n",
    "\n",
    "# plot test_losses_og \n",
    "decoding_loss_only = np.array([(x[0], x[1][1]) for x in test_losses_og])\n",
    "ax2.plot(decoding_loss_only[:, 0], decoding_loss_only[:, 1], '-.', 'b')\n",
    "plt.xlabel('Epoch')\n",
    "ax1.set_ylabel('Norm of change in CNN weights')\n",
    "ax2.set_ylabel('Test loss')\n",
    "plt.title('Norm of change in CNN weights over time')\n",
    "ax1.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
